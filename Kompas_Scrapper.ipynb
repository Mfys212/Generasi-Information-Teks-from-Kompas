{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mfys212/Generasi-Information-Teks-from-Kompas/blob/main/Kompas_Scrapper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-vZBdit7o5-",
        "outputId": "74ee0ee9-a83b-47ea-e177-050ffaaefa3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "scraping |████████████████████████████████| 20/20 [00:22<00:00,  1.13s/it] pages\n"
          ]
        }
      ],
      "source": [
        "import multiprocessing\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "class ScrapKompas:\n",
        "  def __init__(self, tag, max_page=5):\n",
        "    self.page = f\"https://www.kompas.com/tag/{tag}?page=\"\n",
        "    self.link, self.tanggal, self.judul, self.konten = [], [], [], []\n",
        "    self.max_page = max_page\n",
        "    self.max_workers = multiprocessing.cpu_count()\n",
        "\n",
        "  def get_link(self, soup):\n",
        "    links = soup.find_all('a', class_='article__link')\n",
        "    return links\n",
        "\n",
        "  def get_tanggal(self, soup):\n",
        "    try:\n",
        "      try:\n",
        "        time_text = soup.find('div', class_='read__time').get_text()\n",
        "        pattern = r'\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} \\w{3}'\n",
        "        time = re.search(pattern, time_text).group()\n",
        "        date = re.sub(r'(\\d{2})/(\\d{2})/(\\d{4}), (\\d{2}:\\d{2}) (\\w{3})', r'\\3-\\2-\\1 \\4:00', time)\n",
        "      except:\n",
        "        month_mapping = {\n",
        "          'Januari': 'January',\n",
        "          'Februari': 'February',\n",
        "          'Maret': 'March',\n",
        "          'April': 'April',\n",
        "          'Mei': 'May',\n",
        "          'Juni': 'June',\n",
        "          'Juli': 'July',\n",
        "          'Agustus': 'August',\n",
        "          'September': 'September',\n",
        "          'Oktober': 'October',\n",
        "          'November': 'November',\n",
        "          'Desember': 'December'\n",
        "        }\n",
        "        date = soup.find('div', class_='videoKG-date').get_text()\n",
        "        for ind_month, eng_month in month_mapping.items():\n",
        "          date = date.replace(ind_month, eng_month)\n",
        "        date = datetime.strptime(date, \"%d %B %Y, %H:%M WIB\")\n",
        "        wib = pytz.timezone('Asia/Jakarta').localize(date)\n",
        "        date = wib.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    except:\n",
        "      date = \"\"\n",
        "    return date\n",
        "\n",
        "  def get_judul(self, soup):\n",
        "    try:\n",
        "      title = soup.find('h1', class_='read__title').get_text()\n",
        "    except:\n",
        "      title = \"\"\n",
        "    return title\n",
        "\n",
        "  def get_konten(self, soup):\n",
        "    try:\n",
        "      content = soup.find('div', class_='read__content').get_text(strip=True)\n",
        "    except:\n",
        "      content = \"\"\n",
        "    return content\n",
        "\n",
        "  def get_data(self, save=True, output=\"csv\", filename=\"data.csv\"):\n",
        "    def process_link(link):\n",
        "      try:\n",
        "        soup = BeautifulSoup(requests.get(link['href']).content, 'html.parser')\n",
        "        tanggal = self.get_tanggal(soup)\n",
        "        judul = self.get_judul(soup)\n",
        "        konten = self.get_konten(soup)\n",
        "        if judul != \"\" and konten != \"\":\n",
        "          self.link.append(link['href'])\n",
        "          self.tanggal.append(tanggal)\n",
        "          self.judul.append(judul)\n",
        "          self.konten.append(konten)\n",
        "      except:\n",
        "        pass\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n",
        "      futures = []\n",
        "      i = 1\n",
        "      pbar = tqdm(total=self.max_page, ncols=80, bar_format='scraping |{bar}{r_bar} pages')\n",
        "      while True:\n",
        "        page = requests.get(self.page + str(i))\n",
        "        soup = BeautifulSoup(page.content, 'html.parser')\n",
        "        if soup.find('h1', class_='p404__bigtitle'):\n",
        "          print(\"\\nSudah sampai halaman terakhir\")\n",
        "          break\n",
        "        links = self.get_link(soup)\n",
        "        for link in links:\n",
        "          futures.append(executor.submit(process_link, link))\n",
        "        pbar.update(1)\n",
        "        if self.max_page is not None and i == self.max_page:\n",
        "          break\n",
        "        i -= -1\n",
        "      pbar.close()\n",
        "\n",
        "      print(\"Menyelesaikan....\")\n",
        "      for future in as_completed(futures):\n",
        "        future.result()\n",
        "\n",
        "    if save:\n",
        "      if output == \"csv\":\n",
        "        self.save_to_csv(filename=filename)\n",
        "      elif output == \"json\":\n",
        "        self.save_to_json(filename=filename)\n",
        "\n",
        "    return pd.DataFrame({\"Tanggal\":self.tanggal, \"Judul\":self.judul, \"Konten\":self.konten, \"Link\":self.link})\n",
        "\n",
        "  def save_to_csv(self, filename=\"data.csv\"):\n",
        "    data = pd.DataFrame({\"Tanggal\":self.tanggal, \"Judul\":self.judul, \"Konten\":self.konten, \"Link\":self.link})\n",
        "    data.to_csv(filename, index=False)\n",
        "\n",
        "  def save_to_json(self, filename=\"data.json\"):\n",
        "    data = []\n",
        "    for j in range(len(self.judul)):\n",
        "      data.append({\n",
        "        'Tanggal': self.tanggal[j],\n",
        "        'Judul': self.judul[j],\n",
        "        'Konten': self.konten[j],\n",
        "        'Link': self.link[j]\n",
        "      })\n",
        "    with open(filename, 'w', encoding='utf-8') as jsonfile:\n",
        "      json.dump(data, jsonfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "scrapper = ScrapKompas('teknologi', max_page=20)\n",
        "teknologi = scrapper.get_data(save=True, output=\"csv\", filename=\"teknologi.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scrapper2 = ScrapKompas('ai', max_page=1000)\n",
        "ai = scrapper.get_data(save=True, output=\"csv\", filename=\"ai.csv\")"
      ],
      "metadata": {
        "id": "OtLUa39D5NvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrapper3 = ScrapKompas('komputer', max_page=1000)\n",
        "komputer = scrapper.get_data(save=True, output=\"csv\", filename=\"komputer.csv\")"
      ],
      "metadata": {
        "id": "uk2-dZP18sm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrapper4 = ScrapKompas('sains', max_page=1000)\n",
        "sains = scrapper.get_data(save=True, output=\"csv\", filename=\"sains.csv\")"
      ],
      "metadata": {
        "id": "bg9seyfR8yfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrapper5 = ScrapKompas('teknik', max_page=1000)\n",
        "teknik = scrapper.get_data(save=True, output=\"csv\", filename=\"teknik.csv\")"
      ],
      "metadata": {
        "id": "jkwIGVvt84N6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}